{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __call__(self,x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    def gradient(self,x):\n",
    "        return self.__call__(x)*(1 - self.__call__(x))\n",
    "class Softmax():\n",
    "    def __call__(self,x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "    def gradient(self,x):\n",
    "        p = self.__call__(x)\n",
    "        return p * (1-p)\n",
    "class CrossEntropy():\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def loss(self,y,p):\n",
    "        p = np.clip(p,1e-15,1-1e+15)\n",
    "        return -y*np.log(p) - (1-y)*np.log(1-p)\n",
    "    \n",
    "    def gradient(self,y,p):\n",
    "        p = np.clip(p,1e-15,1-1e+15)\n",
    "        return -(y/p) + (1-y)/(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "class dnn():\n",
    "    def __init__(self,n_hidden,n_iter=300,learning_rate=0.1):\n",
    "        self.n_hidden = n_hidden # hidden neurons\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_activation = Sigmoid()\n",
    "        self.output_activation = Softmax()\n",
    "        self.loss = CrossEntropy()\n",
    "    \n",
    "    def _init_weights(self,x,y):\n",
    "        n_sample, n_feature = x.shape\n",
    "        _, n_outputs = y.shape\n",
    "        \n",
    "        #hidden layer\n",
    "        limit = 1 / math.sqrt(n_feature)\n",
    "        self.w = np.random.uniform(-limit,limit, (n_feature, self.n_hidden))\n",
    "        self.w0 = np.zeros((1, self.n_hidden))\n",
    "        \n",
    "        #output layer\n",
    "        limit = 1 / math.sqrt(self.n_hidden)\n",
    "        self.v = np.random.uniform(-limit,limit, (self.n_hidden, n_outputs))\n",
    "        self.v0 = np.zeros((1, n_outputs))\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        self._init_weights(x,y)\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            # forward pass\n",
    "#             print(x.dot(self.w).shape,self.w0.shape)\n",
    "            hidden_input = x.dot(self.w) + self.w0\n",
    "            hidden_output = self.hidden_activation(hidden_input)\n",
    "            \n",
    "            output_layer_input = hidden_output.dot(self.v) + self.v0\n",
    "            y_pred = self.output_activation(output_layer_input)\n",
    "            \n",
    "            #backward pass\n",
    "            grad_wrt_ouput = self.loss.gradient(y, y_pred)*self.output_activation.gradient(output_layer_input)\n",
    "            grad_v = hidden_output.T.dot(grad_wrt_ouput)\n",
    "            grad_v0 = np.sum(grad_wrt_ouput, axis=0, keepdims=True)\n",
    "            \n",
    "            grad_wrt_hidden = grad_wrt_ouput.dot(self.v.T)*self.hidden_activation.gradient(hidden_input)\n",
    "            grad_w = x.T.dot(grad_wrt_hidden)\n",
    "            grad_w0 = np.sum(grad_wrt_hidden, axis=0, keepdims=True)\n",
    "            \n",
    "            self.v -= self.learning_rate*grad_v\n",
    "            self.v0 -= self.learning_rate*grad_v0\n",
    "            self.w -= self.learning_rate*grad_w\n",
    "            self.w0 -= self.learning_rate*grad_w0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        hidden_input = x.dot(self.w) + self.w0\n",
    "        hidden_output = self.hidden_activation(hidden_input)\n",
    "        output_layer_input = hidden_output.dot(self.v) + self.v0\n",
    "        y_pred = self.output_activation(output_layer_input)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"iris.csv\")\n",
    "    y = df['Species']\n",
    "\n",
    "    # y_to_categlorical\n",
    "    le = LabelEncoder().fit_transform(y)\n",
    "    y_to_cate = OneHotEncoder().fit_transform(le.reshape(-1,1))\n",
    "    y = np.array(y_to_cate.toarray())\n",
    "\n",
    "\n",
    "    x = df.drop(['Species','Id'],axis=1)\n",
    "    \n",
    "    x = np.array(x)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,\n",
    "                                                     test_size = 0.2,\n",
    "                                                     random_state = 22)\n",
    "    \n",
    "#     print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "\n",
    "    clf = dnn(n_hidden=32,n_iter=1300,learning_rate=0.01)\n",
    "    clf.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = np.argmax(clf.predict(x_test),axis=1)\n",
    "    y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "    print(accuracy_score(y_pred,y_test))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
