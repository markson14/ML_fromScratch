{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "def load_data(label_name='Species'):\n",
    "    \"\"\"Parses the csv file in TRAIN_URL and TEST_URL.\"\"\"\n",
    "\n",
    "    # Create a local copy of the training set.\n",
    "    train_path = tf.keras.utils.get_file(fname=TRAIN_URL.split('/')[-1],\n",
    "                                         origin=TRAIN_URL)\n",
    "    # train_path now holds the pathname: ~/.keras/datasets/iris_training.csv\n",
    "\n",
    "    # Parse the local CSV file.\n",
    "    train = pd.read_csv(filepath_or_buffer=train_path,\n",
    "                        names=CSV_COLUMN_NAMES,  # list of column names\n",
    "                        header=0  # ignore the first row of the CSV file.\n",
    "                       )\n",
    "    # train now holds a pandas DataFrame, which is data structure\n",
    "    # analogous to a table.\n",
    "\n",
    "    # 1. Assign the DataFrame's labels (the right-most column) to train_label.\n",
    "    # 2. Delete (pop) the labels from the DataFrame.\n",
    "    # 3. Assign the remainder of the DataFrame to train_features\n",
    "    train_features, train_label = train, train.pop(label_name)\n",
    "\n",
    "    # Apply the preceding logic to the test set.\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_features, test_label = test, test.pop(label_name)\n",
    "\n",
    "    # Return four DataFrames.\n",
    "    return (train_features, train_label), (test_features, test_label)\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Report Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df = pd.read_csv('datatraining.txt')\n",
    "train = train_df.drop(['date'],axis=1)\n",
    "label = train_df['Occupancy']\n",
    "train = train.drop(['Occupancy'],axis=1)\n",
    "x_train, x_test,y_train,y_test = train_test_split(train, label ,test_size=0.2)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "model building from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "\n",
    "class LogisticRegression():\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_iterations: int\n",
    "            梯度下降的轮数\n",
    "        learning_rate: float\n",
    "            梯度下降学习率\n",
    "    \"\"\"\n",
    "    def __init__(self,learning_rate, n_iters):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "    def weight_initialize(self, n_features):\n",
    "        limit = np.sqrt(1/n_features)\n",
    "        w = np.random.uniform(-limit,limit, (n_features,1))\n",
    "        b = 0\n",
    "        self.w = np.insert(w, 0, b, axis=0)      \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print('------------------------------------')\n",
    "        print('Start training...')\n",
    "        print('Leanring Rate: {}  Iterations: {}'.format(self.learning_rate,self.n_iters))\n",
    "        print('------------------------------------')\n",
    "        m_samples, n_features = X.shape\n",
    "        self.weight_initialize(n_features)\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y = np.reshape(y, (m_samples, 1))\n",
    "    \n",
    "        for i in range(self.n_iters):\n",
    "            h_x = X.dot(self.w)\n",
    "            y_pred = sigmoid(h_x)\n",
    "            w_grad = X.T.dot(y_pred - y)\n",
    "            loss = np.sum(np.square(y_pred-y))/len(y)\n",
    "            print('In iter{} loss: {}'.format(i,round(loss,4)))\n",
    "            self.w = self.w - self.learning_rate * w_grad\n",
    "            \n",
    "    def predict(self, X):\n",
    "        print('------------------------------------')\n",
    "        print('Start predicting...')\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        h_x = X.dot(self.w)\n",
    "        y_pred = np.round(sigmoid(h_x))\n",
    "        return y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Start training...\n",
      "Leanring Rate: 0.001  Iterations: 20\n",
      "------------------------------------\n",
      "In iter0 loss: 0.7855\n",
      "In iter1 loss: 0.214\n",
      "In iter2 loss: 0.2143\n",
      "In iter3 loss: 0.786\n",
      "In iter4 loss: 0.2143\n",
      "In iter5 loss: 0.1075\n",
      "In iter6 loss: 0.0807\n",
      "In iter7 loss: 0.0846\n",
      "In iter8 loss: 0.082\n",
      "In iter9 loss: 0.0849\n",
      "In iter10 loss: 0.0811\n",
      "In iter11 loss: 0.085\n",
      "In iter12 loss: 0.0718\n",
      "In iter13 loss: 0.1007\n",
      "In iter14 loss: 0.1076\n",
      "In iter15 loss: 0.1064\n",
      "In iter16 loss: 0.1874\n",
      "In iter17 loss: 0.2099\n",
      "In iter18 loss: 0.786\n",
      "In iter19 loss: 0.2145\n",
      "------------------------------------\n",
      "Start predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(0.001,20)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9343155310006138"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
